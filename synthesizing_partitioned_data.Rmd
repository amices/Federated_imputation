---
title: "Federated Imputation"
author: 
- Thom Volker
- Utrecht University
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
bibliography: federated_imp.bib
csl: "/Users/thomvolker/Documents/styles/apa-6th-edition.csl"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
thomcache <- TRUE; thomlazy <- FALSE #obviously ;)
library(tidyverse)
```

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.d Synthesize partitioned data.R")
```


# Previous results

Without partitioning the data, we already saw that synthesizing does not distort the relationships of the variables in the data at hand. That is, synthesizing a dataset results in $M$ synthetic datasets with, on average, the same properties as the original dataset. The results can be seen in the following table. This table contains the simulation results of synthesizing multivariate normally distributed data, where the correlation between predictors equals $\rho = 0$, the proportion explained variance equals $R^2 = .50$ and the sample size equals $n = 100$. Note that synthesizing is done by means of the default settings (by means of CART), and the variance of the estimates is calculated as 
$$Var(\beta) = \bar{v}_{M}(1 + \frac{1}{M}),$$
where $\bar{v}_M$ is the average variance of the regression coefficient over the $M$ synthetic datasets.

```{r, echo = F}
knitr::kable(summary_out_cart, digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data with m=5 in the last three rows.")
```

# Results with partitioned data and uncorrelated predictors

## CART Synthesizing method

CART Method: First variable is sampled from the observed data, the second variable is predicted from the first variable by means of a regression tree, the third variable is predicted from the first two variables, and so on. 

When we first partition the sampled data from the multivariate normal distribution, the results change to quite some extent, which can be seen in the following table. The coverage seems to be very low, and far below the nominal level of $95\%$. 


```{r, echo = F}
bind_rows("n = 100" = summary_out_cart_part_n100, 
          "n = 1000" = summary_out_cart_part_n1000,
          "n = 10000" = summary_out_cart_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```


## Norm Synethesizing method

Norm Method: The first variable is sampled from a univariate normal distribution, the second variable is drawn from a normal distribution with mean $\mu = \beta_1 * X_1$ and variance $\sigma^2 = \sigma^2_e$, the third variable is drawn based on the first two variables, and so on. This performs much better than the previous method, but this introduces bias, which is accompanied by a somewhat too low coverage.

```{r, echo = F}
bind_rows("n = 100" = summary_out_norm_part_n100,
          "n = 1000" = summary_out_norm_part_n1000, 
          "n = 10000" = summary_out_norm_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of method 'norm' with M = 10 in the last three rows with partitioned data.")
  

```


# Results with partitioned data and correlated predictors

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.e Synthesize partitioned data with correlations.R")
```

When there are no correlations between the predictors, there is less information in the data with regard to every predictor, so it might be more difficult to correctly estimate all relationships in the data appropriately. Therefore, we introduce some moderate correlations between the predictors, so that, hopefully, the patterns in the data can be used to synthesize the data. The following correlation matrix is used to model the relationships between the data.

$$
\rho =
\begin{bmatrix}
1 & 0.15 & 0.25\\
0.15 & 1 & 0.35\\
0.25 & 0.35 & 1
\end{bmatrix}
$$

## CART Synthesizing method

When we use the CART synthesizing method, in combination with the data with correlated predictors coming from a multivariate normal distribution, we obtain the following results.

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
bind_rows("n = 100" = cor_summary_out_cart_part_n100, 
          "n = 1000" = cor_summary_out_cart_part_n1000,
          "n = 10000" = cor_summary_out_cart_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

## Norm Synthesizing method

When we use the norm synthesizing method, in combination with the data with correlated predictors coming from a multivariate normal distribution, we obtain the following results.

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
bind_rows("n = 100" = cor_summary_out_norm_part_n100, 
          "n = 1000" = cor_summary_out_norm_part_n1000,
          "n = 10000" = cor_summary_out_norm_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

# Ten subsets instead of five

## Uncorrelated predictors

### CART Synthesization

Below, the results are presented when the data is partitioned into 10 subsets, instead of five, corresponding to ten different nodes that are synthesized separately, and then combined into a single dataset that is analysed by means of the standard estimators as presented by @raab_practical_2016. This yields regression coefficients that are equal to
$$
\beta_{M,k} = \sum^M_{m=1} \beta_{m,k} / M,
$$
with a variance of 
$$
V = \bar{v}_M(1+\frac{1}{M}),
$$
where $\bar{v}_M$ equals $\sum^M_{m=1}\frac{v_m}{M}$.

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.f Synthesize 10 subsets with and without correlations.R")
```


```{r, echo = F}
bind_rows("n = 100" = p10_summary_out_cart_part_n100, 
          "n = 1000" = p10_summary_out_cart_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

### Norm synthesization

```{r, echo = F}
bind_rows("n = 100" = p10_summary_out_norm_part_n100, 
          "n = 1000" = p10_summary_out_norm_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

## Correlated predictors

### CART synthesization

```{r, echo = F}
bind_rows("n = 100" = cor_p10_summary_out_cart_part_n100, 
          "n = 1000" = cor_p10_summary_out_cart_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

### Norm synthesization

```{r, echo = F}
bind_rows("n = 100" = cor_p10_summary_out_norm_part_n100, 
          "n = 1000" = cor_p10_summary_out_norm_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

# Workflow from now on

Necessity to further investigate structure of the data by means of, say, eigenvalues (correlations seem to be okay with large samples, according to results presented)?

## Proposal

### Complete data workflow

@. Verder werken met de completed boys data (ondanks de hoge correlatie sowieso realistischer dan gesimuleerde multivariate normal data). Eventueel zou ik ook nog een andere dataset kunnen zoeken die wellicht meer lijkt op een "standaard" social sciences dataset.
  + Indien we met de boys data werken, kunnen we eventueel ook nog een subset nemen (om te kijken hoe het gaat met "echte" data als we de sample size verkleinen), en we zouden kunnen bootstrappen om een grotere sample te creÃ«ren dan de huidige 748 observaties (wellicht met Stef overleggen wat een redelijk aantal observaties en nodes zijn over verschillende ziekenhuizen).
@. For i in 1:nsim (en eventueel voor verschillende sample sizes):
    + Boys dataset (of dus eventueel andere) bootstrappen, zodat we een "superpopulatie" krijgen. 
    + Partition bootstrap sample, zodat we weer nodes krijgen.
    + Synthesize nodes independently from each other.
    + Rbind synthetic versions.
    + Analyse de complete (n = 748) synthetic dataset en de gebootstrapte dataset. 
@. Dan: gemiddelde over de bootstraps is de "populatiewaarde".
@. Check Qbar, bias, CI width en CI coverage van synthetische data.

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.g Synthesize bootstrapped boys data.R")
```

The results we obtain in this scenario are shown in the following output. First the variable `age` is sampled randomly from the actual data, then the variable `hgt` is synthesized, with only `age` as predictor. In the same fashion, `wgt` is synthesized with predictors `age` and `hgt`, and so on so forth. It can be seen that the confidence interval coverage of the bootstrapped sample is not optimal, and that the coverage of the synthetic data after $M = 10$ synthetic versions is actually nearly identical to that of the bootstrapped samples. 

```{r, echo = F}
knitr::kable(summary_cart_10, digits = 3)
```

```{r, echo = F, dpi = 300}
boys_cart_results_10 %>%
  filter(Variable == "(Intercept)") %>%
  ggplot(mapping = aes(x = Est, color = Method, fill = Method)) +
  geom_density(alpha = .4) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("Intercept") +
  theme_classic() -> plot_intercept
  
boys_cart_results_10 %>%
  filter(Variable == "age") %>%
  ggplot(mapping = aes(x = Est, color = Method, fill = Method)) +
  geom_density(alpha = .4) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("Age") +
  theme_classic() -> plot_age
  
boys_cart_results_10 %>%
  filter(Variable == "hgt") %>%
  ggplot(mapping = aes(x = Est, color = Method, fill = Method)) +
  geom_density(alpha = .4) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("Height") +
  theme_classic() -> plot_height

cowplot::plot_grid(cowplot::plot_grid(plot_intercept + theme(legend.position = "none"), 
                                     plot_age + theme(legend.position = "none"), 
                                     plot_height + theme(legend.position = "none"), 
                                     nrow = 1, align = 'vh'),
                  cowplot::get_legend(plot_age + theme(legend.position = "bottom")),
                                      ncol = 1, rel_heights = c(1, .1))
```


When we increase the number of synthetic datasets to $M = 20$, the coverage is actually slightly worse than in the case of $M = 10$. This seems to be due to the fact that the bias remains and the confidence interval gets slightly narrower. 

```{r, echo = F}
#knitr::kable(summary_vs_10, digits = 3)

knitr::kable(summary_cart_20, digits = 3)

#knitr::kable(summary_vs_20, digits = 3)
```

<!--Then we have the actual sample results on the complete boys dataset, without synthesizing. -->

```{r, include = F}
knitr::kable(broom::tidy(lm(wgt ~ age + hgt, boyscomp), conf.int = T))
```

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.h Synthesize bootstrapped boys7482 data.R")
```

When we additionally do this with the `boys7482` data from the package `AGD`, we get slightly beter results. However, there remains a problem with regard to the bootstrapped samples from the population. So apparently, the bootstrapping procedure induces more variance than would be expected based on the $95\%$ CI. I am not sure why this error is introduced. Additionally, the synthesizing method seems to be somewhat biased, which can be seen in the density plots displayed after the tables.

```{r, echo = F}
knitr::kable(summary_cart_10_7482, digits = 3)
```

```{r, echo = F, dpi = 300}
boys_cart_results_10_7482 %>%
  filter(Variable == "(Intercept)") %>%
  ggplot(mapping = aes(x = Est, color = Method, fill = Method)) +
  geom_density(alpha = .4) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("Intercept") +
  theme_classic() -> plot_intercept_7482
  
boys_cart_results_10_7482 %>%
  filter(Variable == "age") %>%
  ggplot(mapping = aes(x = Est, color = Method, fill = Method)) +
  geom_density(alpha = .4) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("Age") +
  theme_classic() -> plot_age_7482
  
boys_cart_results_10_7482 %>%
  filter(Variable == "hgt") %>%
  ggplot(mapping = aes(x = Est, color = Method, fill = Method)) +
  geom_density(alpha = .4) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("Height") +
  theme_classic() -> plot_height_7482

cowplot::plot_grid(cowplot::plot_grid(plot_intercept_7482 + theme(legend.position = "none"), 
                                     plot_age_7482 + theme(legend.position = "none"), 
                                     plot_height_7482 + theme(legend.position = "none"), 
                                     nrow = 1, align = 'vh'),
                  cowplot::get_legend(plot_age_7482 + theme(legend.position = "bottom")),
                                      ncol = 1, rel_heights = c(1, .1))
```


### Missing data workflow

#### IDEA

* Since synthpop messes up 

@. Werken met boys data met missings.
@. Maak superpopulatie obs van bootstrappen (binnen de for-loop)
@. For i in 1:nsim (eventueel voor verschillende sample sizes):
    + Bootstrap boys data (hebben we bootstrap sample)
    + Partitioneer bootstrap sample in 5 delen
    + Synthesize each node independently from the others.
    + Merge the nodes
    + Impute the synthetic dataset as a whole
    + Subset the synthetic data so that they correspond to the observed (partitioned) data
    + Match the corresponding synthetic imputation to the observed imputation for all of the synthetic imputed sets.
@. Gemiddelde over de bootstraps is de populatiewaarde.
@. Check Qbar, bias, CI width en CI coverage van synthetische data.

### IK MOET LEZEN
@. Drechsler over synthetic data
@. Over CART (Elements of Statistical Learning?)


