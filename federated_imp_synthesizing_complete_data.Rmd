---
title: "Federated Imputation"
author: 
- Thom Volker
- Utrecht University
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
bibliography: federated_imp.bib
csl: "/Users/thomvolker/Documents/styles/apa-6th-edition.csl"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
thomcache <- TRUE; thomlazy <- FALSE #obviously ;)
```

# Synthesizing

To show the concept of synthesizing data, the `boys` dataset from the R-package `mice` [@mice] is singly-imputed, to be able to work with a dataset without missing values. The first approach is to synthesize this dataset as a whole by means of the `syn`-function with default settings from the package `synthpop` [@synthpop], to see whether synthesizing is a suitable approach to generate data that is in accordance with the observed data, and exhibits the same relationships between variables. To see how this process performs when the data is partitioned, the data is first partitioned into five subsets. Then, the subsets are synthesized by means of the `syn` function, and then binded to a single dataset containing the five synthesized subsets. Since the `syn` function contains a random process, this function is rerun over a single partitioned dataset, to see what the effect of only randomly synthesizing the same partitions is. As a third step, the partitioning is also randomized, resulting in the fact that the iterations contain two random processes: (1) the data is partioned randomly for every iteration, and (2) the partitioned data is synthesized with a random component.

That is, the process works as follows: 

@. Impute the `boys` dataset with `m=1`, which results in the completed `boyscomp` data.
@. Over `i` in `1:nsim(=1000)` iterations, the output following output is collected:
  + The regression estimates of `nsim` times synthesized complete dataset.
  + The regression estimates of `nsim` times the synthesized partitioned dataset.
  + The regression estimates of `nsim` times the synthesized `nsim` times partitioned dataset.
@. Over `nsim` iterations, the average estimates are calculated ($\bar{Q}$), the bias ($\mu_Q - \bar{Q}$), the confidence intervals averaged over all iterations, and the coverage ($\frac{1}{nsim} \sum_{i = 1}^{nsim} LLCI_i < \mu_Q < ULCI_i$).

# Results

The previous process results in the following results.

```{r, include = F, cache = thomcache, cache.lazy=thomlazy}
source("2.a Synthesize data (m=1).R")
```

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
knitr::kable(syn1_out, digits = 3, caption = "Simulation results of the model age ~ hgt + tv for three different synthesizing approaches (complete data, same subset 1000 times, 1000 different subsets, all with the number of synthetic datasets equal to 1.")
```


However, the results are not very satisfactory, so I looked further into the possibilities of using the `synthpop` package. One of the possibilities was to adjust the model, and use passive synthesizing for `bmi`. Additionally, instead of one synthetic dataset, I made 5, so that the results are averaged over five synthetic datasets, instead of only one. This leads to much better results, as is shown below (only for the case in which the complete dataset is synthetized).

```{r, include = F, cache = thomcache, cache.lazy=thomlazy}
source("2.b Synthesize data (m=5).R")
```


```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
knitr::kable(syn5_out, digits = 3, caption = "Simulation results of the model age ~ hgt + tv based on synthesizing the data as a whole, based on 5 synthetic datasets per iteration.")
```

Which provides much better results. However, I do not know yet how to implement this approach for five partitions in combination with the `synthpop` package.


# Synthpop estimates

Synthetic data estimates $(\hat{\beta_1}, \hat{\beta_2}, \dots, \hat{\beta_m})$ - if the model for the data is correct, the $m$ estimates from the synthetic data will be centered around the estimate $\hat{\beta}$ that would have been obtained from the observed data. If the method of inference used to fit the model provides consistent estimates of the parameters and the same is true for the analyses of the synthetic data, then the mean of $m$ synthetic estimates, $\bar{\hat{\beta}}=\sum_{i=1}^m\hat{\beta_i}/m$ provides a consistent estimate of $\hat{\beta}$. Provided that the observed and synthetic data are generated by the common sampling scheme, then $V_{\bar{\hat{\beta}}}=\sum_{i=1}^m V_{\hat{\beta_i}}/m$ will be a consistent estimate of $V_{\hat{\beta}}$.

The variance-covariance matrix of $\bar{\hat{\beta}}$, conditional on $\hat{\beta}$ and $V_{\hat{\beta}}$, becomes $V_{\hat{\beta}}$ which can be estimated from $V_{\bar{\hat{\beta}}}/m$.  Thus the stochastic error in the mean of the synthetic estimates aboutthe values from the observed data can be reduced to a negligible quantity by increasing $m$. It must be remembered, however that the consistency of $\bar{\hat{\beta}}$ only applies when observed data are a sample from the distribution used for synthesis. In practical applications  differences between the analyses on the observed data and those from the mean of the syntheses will be found because the data do not conform to the model used for synthesis.  Such differences will not be reduced by increasing $m$. The synthesiser, with access to the observed data, can estimate $\bar{\hat{\beta}}-\hat{\beta}$ and compare it to its standard error in order to judge the extent that this model mismatch affects the estimates.

# Multivariate normal simulated data

```{r, include = F, cache = thomcache, cache.lazy=thomlazy}
source("2.c Synthesize data mvnorm.R")
```

## Inference with sample variance estimates

Sample variance estimates are computed as $V_{\bar{\hat{\beta}}} = \frac{1}{m} \sum_{i=1}^m V_{\hat{\beta}_i}$. However, confidence interval coverage is computed with respect to the true population value, instead of the sample estimates.

### Synthesizing by a linear regression model

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
knitr::kable(synth_by_lm, digits = 3, caption = "Simulation results of multivariate normal data, synthesized by a normal linear regression model.")
```

### Synthesizing by a Classification and Regression Tree (CART)

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
knitr::kable(synth_by_cart, digits = 3, caption = "Simulation results of multivariate normal data, synthesized by classification and regression trees.")
```

## Inference with sample variance estimates

Population variance estimates are computed as <!-- TO DO - ADD CORRECT FORMULA $V_{\bar{\hat{\beta}}} = \frac{1}{m} \sum_{i=1}^m V_{\hat{\beta}_i}$ -->. However, confidence interval coverage is computed with respect to the true population value, instead of the sample estimates.

### Synthesizing by a linear regression model

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
knitr::kable(synth_by_lm_pop, digits = 3, caption = "Simulation results of multivariate normal data, synthesized by a normal linear regression model.")
```

### Synthesizing by a Classification and Regression Tree (CART)

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
knitr::kable(synth_by_cart_pop, digits = 3, caption = "Simulation results of multivariate normal data, synthesized by classification and regression trees.")
```


### Variance estimates

The variance of the statistic of interest equals $\bar{v}_M$, which represents the average variance over the $M$ synthetic datasets, when the goal of the synthesis is to make inferences with regard to the sample at hand, instead of with regard to the population. When the goal of the synthesis is to make population inferences, the variance is adjusted with an additional term, based on the number of synthetic datasets: 
$$
Var(Q) = \bar{\mathcal{v}}_M (1 + \frac{1}{M})
$$
where $Q$ is the regression coefficient of the $k^{th}$ variable, $\bar{v}_M$ is the mean of the variables over the $M$ syntheses. If the number of synthesized data points ($J$) is unequal to the number of observations in the dataset ($N$), a correction equal to $\frac{J}{N}$ is multiplied with $\bar{v}_M$. 


# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\noindent

<div id="refs"></div>


