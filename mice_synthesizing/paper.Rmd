---
title: "Outline paper - Multiple imputation for statistical disclosure control: Creating synthetic datasets with *mice*"
subtitle: "Anony*mice*d shareable data: Using *mice* to create multiply imputed synthetic datasets" 
author:
- Thom Volker, Gerko Vink & Stef van Buuren
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
bibliography: federated_imp.bib
csl: "apa-6th-edition.csl"
---

# OUTLINE

## Introduction

- [x] What is synthetic data?
- [x] Why is it useful?
- [x] Why should it be easy to generate?
- [x] Parallel to missing data
- [x] If similar to missing data, then why not use `mice`?
- [x] `mice` can only be exploited if the statistical properties are correct.
- [x] In this paper we demonstrate a straightforward workflow for generating synthetic datasets and we investigate how to obtain valid inferences from synthesized data.

## Generating synthetic data with `mice`

- [ ] Explain FCS in short.
- [ ] Demonstrate procedure with `where-matrix` and explain rationale.
- [ ] Demonstrate that this is a reasonable approach, based on a figure that shows the observed and synthetic relationships side-by-side (e.g. `wgt ~ age + gen`).
- [ ] Observed values can be regarded as starting values for the imputation model. Since the observed data is used for creating the imputation models, no iterations are required. That is, the model is fixed, and not dependent on the imputed values.

## Drawing inferences from `mice` synthesized data

- [ ] What ways are there (with minimal - if any - escapes to partially/fully synthetic data).
- [ ] If based on your earlier work, cite the online reference for your simulations and give URL. No need to go into detail about it here.

### Simulation

- [ ] Simulation set-up.
- [ ] Evaluation criteria explained: the bias, coverage and CIW triangle.
- [ ] Table which demonstrates bias, coverage and CIW for all methods.

## Discussion

- [ ] Summarize findings.
- [ ] Stress that only holds for sampled data. Explain that for fully observed populations, one would need no sampling variance.
- [ ] Revisit the simplicity and the readily available solution within the `mice` workflow (if adjusted pool is required, add to mice and direct to that function).


# Introduction

Open science, including open data, has been marked as the future of science [@gewin_data_2016], and the advantages of publicly available research data are numerous [@molloy_open_2011; @walport_brest_sharing_2011]. Because collecting research data requires an enormous investment both in terms of time and monetary resources, openly accessible research data bears the potential of increasing the scientific returns. Additionally, the fact that public funds are used for data collection results in increased demands for the collected data. Nevertheless, the possibilities to share research data directly are often very limited due to restrictions with regard to privacy and confidentiality. Simply anonimizing the data is not enough to fulfil these requirements [@ohm_broken_2009]. Over the years, several other techniques have been used to increase the confidentiality of the data, such as categorizing continuous variables, top coding values above a certain threshold or adding random noise [@drechsler_synthetic_2011]. However, these methods may distort relationships between variables, thereby drastically reducing the data quality and the usability for further research.

An alternative solution has been proposed separately by @rubin_statistical_disclosure_1993 and @little_statistical_1993. Although their approaches differed to some extent, the general idea is to use actually observed data to generated multiply imputed synthetic datasets that can be freely disclosed. Based on some imputation model, values from the actually observed data are replaced by multiple draws from the posterior predictive distribution of the observed data. Using this approach, the researcher could replace the observed data set as a whole with multiple synthetic versions. Alternatively, the researcher could opt to only replace a subset of the observed data. The values that are to be replaced can be complete variables that could be compared with publicly available data sets or registers, such as addresses, or can be certain values that bear a high risk of being disclosive, say, income values above a certain threshold.

Conceptually, the synthetic data framework is based upon the building blocks of multiple imputation of missing data, as proposed by @rubin_multiple_1987. Instead of replacing just the missing values with multiple draws from the posterior predictive distribution, one could easily *overimpute* any observed sensitive values. Similarly to multiple imputation of missing data, the multiple synthetic datasets allow for correct statistical inferences. The results of the multiply synthesized data sets should be pooled into a single inference, so that the researcher can draw valid inference from a single pooled analysis. To that respect, the variance should reflect the added variability that is induced by the imputation procedure. 

Potentially, this approach could fulfill the needs for openly accessibly data, without running into barriers with regard to privacy and confidentiality constraints. However, there is no such thing as a free lunch: data collectors have to put effort in creating high-quality synthetic data. Also, the quality of the synthetic data are highly dependent on the imputation models, and using flawed models to generate synthetic data might bias subsequent analyses. Conversely, if the models used to create the synthetic data are able to preserve the relationships between the variables as in the original data, the synthetic data can be nearly as informative as the observed data. Thus, to fully exploit the benefits of synthetic data, the effort to actually create these datasets should be kept at a minimum.

To minimize additional effort of creating synthetic datasets on behalf of the researcher, software aimed at multiple imputation of missing data can be employed. Especially if researchers used this software at an earlier stage in the research process, or acquired familiarity with it during earlier projects, the additional burden of creating synthetic datasets is relatively small. The R-package `mice` [@mice] implements multiple imputation of missing data in a straightforward and user-friendly manner. However, the functionality of `mice` is not restricted to the imputation of missing data, but allows to impute any value in the data. Thus, `mice` can be utilized for the creation of multiply imputed synthetic datasets. 

After creating the multiply imputed synthetic data sets, the goal is to obtain valid statistical inferences from them. In the missing data framework, this is done by performing statistical analyses on all imputed data sets, and pooling the results of the analyses according to Rubin's rules [-@rubin_multiple_1987]. In the synthetic data framework, the same procedure is followed. However, valid statistical results can only be obtained under the premise that the statistical properties of the analyses are correct. Analyzing synthetic data specically requires appropriate variance estimates. Notably, the variance estimator that is suitable for analyzing multiply imputed datasets for missing data is inappropriate when multiply imputed synthetic datasets are analyzed. As a result, multiple variance estimators have been proposed to analyze multiply imputed synthetic datasets correctly, that differ according to whether the data consists entirely of synthetic values, or only a subset of the data is synthetic.

The remainder of this paper is organized as follows. First, we will shortly describe the `mice` algorithm for the creation of synthetic data that reassures the privacy and confidentiality of the participants. Second, we will demonstrate a straightforward workflow for imputation of synthetic data with `mice`. The third step is to summarize the variance estimators that have been proposed in the past. Finally, we will assess which variance estimator provides valid inferences from the synthetic data.

# Generating synthetic data with `mice`

The `mice` package has been developed originally for multiple imputation of missing data. In this context, the aim is to replace missing values due to nonresponse by plausible values from the posterior predictive distribution of the variable containing the missing values. Doing so, `mice` makes use of fully conditional specification (FCS), which breaks down the multivariate distribution of the data $\textbf{Y} = (\textbf{Y}_{obs}, \textbf{Y}_{mis})$ into $j = 1, 2, \dots, k$ univariate conditional densities, with $k$ the number of variables in the data. Using FCS, for every variable containing missing values a model is constructed, to impute the missing values $Y_{j, mis}$ with draws from the posterior predictive distribution of $P(Y_{j, mis} | \textbf{Y}_{obs}, \theta)$ on a variable by variable basis. Note that the predictor matrix $Y_{-j}$ may contain yet imputed values from an earlier imputation step, and thus will be updated after every iteration. This procedure is applied $m$ times, resulting in $m$ completed datasets $\textbf{D} = (\textbf{D}^{(1)}, \textbf{D}^{(2)}, \dots, \textbf{D}^{(m)})$, with $\textbf{D}^{(l)} = (\textbf{Y}_{obs}, Y^{(l)}_{mis}$. __MAYBE ADD AN EXAMPLE OF HOW TO DO IT IN `mice`__

__DISCUSS ESTIMATORS FOR MISSING DATA HERE, OR AT A LATER POINT__

This approach can be straightforwardly extended to a synthetic data framework. However, rather than imputing missing data, we overimpute actually observed data, that is, we replace observed values by multiple imputations. For simplicity, assume that the data is completely observed (i.e., $\textbf{Y} = \textbf{Y}_{obs}$). Following the notation of @reiter_raghunathan_multiple_2007, let $Z_j = 1$ if any of the values of unit $j$ are to be replaced by imputations, and $Z_j = 0$ otherwise, with $\textbf{Z} = (Z_1, Z_2, \dots, Z_n)$. Accordingly, the data consists of values that are to be replaced and values that are to be kept (i.e., $\textbf{Y}^{(l)} = (\textbf{Y}^{(l)}_{rep}, \textbf{Y}_{nrep})$. Note the superscript $^{(l)}$ indicating the $l$th synthetic data set. Now, instead of imputing $\textbf{Y}^{(l)}_{mis}$ with draws from the posterior predictive distribution of $P(Y_{j, mis} | \textbf{Y}_{obs}, \theta)$ as in the missing data case, we impute $\textbf{Y}^{(l)}_{rep}$ from the posterior distribution of $(Y_{rep} | \textbf{Y}, \textbf{Z}, \theta)$. This process results in $\textbf{D} = (\textbf{D}^{(1)}, \textbf{D}^{(2)}, \dots, \textbf{D}^{(m)})$, with $m$ the number of synthetic datasets. 

By default, `mice` imputes all values that are coded as missing in the dataset. Using the `where` parameter within the mice function, one can choose to impute any value in the dataset. As input, the `where` parameter requires a matrix of the same dimensions of the data, (i.e., a $n \times k$ matrix, with $n$ the number of observations in the collected data) containing logical values.
<!--

Now, we aim to generate the replacement values for the actually observed data $Y_{syn}$, instead of values that replace missing values $Y_{imp}$. 

Using FCS, for every variable containing missing values, a model is constructed to impute the missing values on a given variable conditional on all other variables in the data, and the posterior distribution of the parameters used in the model at hand, on a variable by variable basis, which yields $P(Y_{mis} | X, \theta)$. 


The `mice` package has been developed originally for multiple imputation of missing data, but can be easily extended to the synthetic data framework. It proceeds by drawing missing or synthetic values from the joint distribution of the observed data. This joint density is generally hard or impossible to derive, which can be overcome by employing fully conditional specification (FCS). Consider a $n \times k$ data matrix $Y$, with values to be replaced $Y^{old}$, the values to be kept $Y^{keep}$ and the values that are already replaced $Y^{new}$, under the assumption that there are no missing values in the observed data set. Instead of drawing from the joint distribution, we draw from $k$ univariate conditional densities for each variable. That is, we draw from the distribution $P\big{(}Y_j^{old} | Y^{keep}, Y^{new}, Y^{old}_{-j}, \theta \big{)}$, where the drawn values are conditional upon the values that are to be kept, the synthetic values that are drawn at an earlier iteration and the values that still are to be replaced, with $j = 1, 2, \dots, k$ and $\theta$ indicating the parameter specifying the distribution of $Y_j^{old}$. Thus, the synthetic values are generated variable by variable. This procedure is executed $i = 1, 2, \dots, m$ times, resulting in $\textbf{D} = \{D^{(1)}, \dots, D^{(m)}\}$ synthetic datasets, with $D^{(i)} = (Y^{keep}, Y^{i, new})$.

To select the values that are to be replaced, one can utilize the `where` parameter in the `mice` function. This parameter allows one to set which values of the data are to be imputed. 

-->






KEY: Het moet niet mogelijk zijn om de echte waarden te reverse engineeren. De stochastische component van multiple imputation zorgt er voor dat dit onmogelijk is. Wat wel kan worden afgeleid is het ware data genererende model - en dat is nou precies de bedoeling. 



<!--

It is easy to draw



The goal then is to release multiply imputed datasets that bear no risk of releasing confidential or identifying information, while the multivariate relationships should be preserved as in the observed data.




The fact that the then released data is synthetic could potentially fulfil the needs for openly accessible data, without running into barriers with regard to privacy and confidentiality constraints.


, based on the multiple imputation framework [@rubin_multiple_1987]. 


Potentially, these approaches  However, there is no such thing as a free lunch: data collectors have to put effort in creating synthetic data, and using incorrect models to generate synthetic data might yield biased results for potential analysts of the data. It thus is crucial that the synthesizing process will be as easy as it gets, so that the step to release synthetic data is as small as possible.


An alternative has been offered by @rubin_statistical_disclosure_1993, who, building on the framework of multiple imputation, proposed to release multiple synthetic datasets to the public. Conform this approach, all units that are in the population but not in the sample are treated as missing data. These values are imputed by means of conventional multiple imputation approaches, and simple random samples are drawn from the population. Ultimately, these samples are released to the public. In practice, it is not required to impute the complete population, since random samples can be drawn from the sampling frame, so that only these sampled values have to be imputed. The datasets generated under this approach are labelled as *fully synthetic datasets*. However, note that, if one draws simple random samples from the sampling frame, the possibility exists that actual observations are included. 

A second procedure to creating synthetic datasets has been proposed by @little_statistical_1993, named *partially synthetic datasets*, which originated as a procedure that required synthesizing only those values that are at a high risk of being disclosive. These values can be complete variables that could be compared with publicly available datasets or registers, such as addresses, or it can be certain values that bear a high risk of being disclosive, say, income values above a certain threshold. Similarly to the fully synthetic data approach, multiple datasets containing synthetic values are released to the public, although in this instance, only the values at a high risk vary over the synthetic datasets. However, nothing keeps us from treating all values in the dataset as bearing a high disclosure risk, resulting in datasets that are completely existing of synthetic values. This approach basically implements the idea of fully synthetic data, based on the partially synthetic data procedure.

-- -- HIER MIST NOG EEN OVERGANG WAAR IK NOG NIET UIT BEN -- -- 


As of today, most papers in this field address methodological issues concerning both fully and partially synthetic datasets, both with regard to analysis methods suitable for creating synthetic data, and concerning inferential procedures. The current paper aims at reducing the gap between the theory about synthetic data, and generating synthetic data in practice. Due to the conceptual similarities between imputation for missing data and imputation for synthetic data, we propose that the R-package `mice` is capable of bridging the gap between theory and actual applications. Originally, `mice` is developed as an R-package to impute values subject to non-response. However, algorithmically, the procedure of imputing values subject to non-response does not need to differ from imputing synthetic data. Both approaches can be based on fully conditional specification (FCS) as implemented in `mice`. Conform this approach, values are drawn iteratively from the posterior predictive distribution of the variable of interest conditional on all other variables, for each variable separately. Thus, previously imputed values are taken into account in the imputation model for variables that are imputed later on. 

Since `mice` is currently only capable of imputing the data at hand, the approach should be labelled as *partially synthetic*. That is, although it is completely possible to create a synthetic dataset that consists of only synthetic values, `mice` is not (yet) capable of imputing observations that are not present in the data. Creating fully synthetic datasets by means of `mice` remains an area of future research.


-- -- NOT SURE YET: CONTRAST FULLY AND PARTIALLY SYNTHETIC DATA -- --

-- -- SINCE PARTIALLY SYNTHETIC DATA POOLING RULES DO NOT REQUIRE ADJUSTED VARIANCE ESTIMATES -- --

-- -- CAN WE REGARD THE OBSERVED VALUES AS THE STARTING VALUES FOR THE IMPUTATIONS ? -- --

<!-- The R-package `mice` [@mice] implements the fully conditional specification approach for missing data. Since the imputation approaches for missing data and synthetic data do not differ in terms of the imputation process, `mice` can be used for creating synthetic data as well, which will be shown in more detail in the remainder of this paper. In the next section, the pooling rules that are required to obtain inferences from the synthetic datasets are discussed. Thereafter, it will be investigated which variance estimator yields the best, that is, confidence valid, results in combination with the `mice` synthesizing approach. Also, it is shown that iterating over the conditional distributions in order to establish convergence is not required, due to the fact that new values are drawn directly from posterior distribution given the observed data. -->

**Intro**

- Motivation/relevance

- Synthetic data (fully/partially shortly explained)

- How can this data be generated in practice (FCS - multivariate synthetic values)

- Mice adopts FCS for missing data

- It will be shown that mice is also capable of generating multivariate synthetic data

- However, multiple rules for correct variance estimates have been proposed, but the choice of one optimal variance estimate is not straightforward. Therefore, the performance of the currently available variance estimators (as outlined below) will be contrasted

- Goal: explain and show how mice can be used to create synthetic versions of the data at hand, and which variance estimator yields the best results.

**Synthetic data explained**

*Fully synthetic data*

- Explain the idea in more depth than in the intro
- Explain pooling rules by Raghunathan, Reiter & Rubin
- Adjusted variance estimate by Reiter (restricted positive)
- Simple variance estimator by `synthpop` authors.

*Partially synthetic data*

- Explain the idea in more depth than in the intro
- Explain pooling rules by Reiter.
- Adjusted estimate by `synthpop` authors.

**Mice**

 - Explain algorithm (FCS) in more depth
 - What is synthetic data
 - Why is it useful
 - Why should it be easy to generate
 - Parrallel to missing data
 - If similar to missing data then why not use mice
 - All fun and game, but statistical properties should be intact
 - In this paper we demonstrate a straigthforward workflow for generating synthetic data sets and we investigate how to obtain valid inferences from synthesized data. 
 
## Generating synthetic data with mice
- Explain FCS in short
- Demonstrate procedure with where matrix and explain rationale
- Demonstrate that it is a reasonable approach based figure that shows obs relations and synth relations side-by-side (e.g. wgt ~ age | gen)
- Observed values can be regarded as starting values for the imputation model. Since the observed data is used for creating the imputation models, no iterations are required. That is, the model is fixed, and not dependent on the imputed values.

## Drawing inferences from mice synthesized data
- What ways are there (with minimal - if any - escapes to partial/fully synthesized)
- If based on your earlier work, cite the online reference for your simulations and give URL. No need to go into detail about it here. 

### Simulation
- Simulation set-up
- Evaluation criteria explained: the bias cov ciw triangle
- Table with demonstrate bias cov ciw for all methods

## Discussion
- Summarize findings
- Stress that only holds for sampled data. Explain that for fully observed populations, one would need no sampling variance. 
- Revisit the simplicity and the readily available solution within the mice workflow (if adjusted pool is required, add to mice and direct to that function)


# FULLY/PARTIALLY synthetic data

## Fully synthetic data

DISCUSS APPROACH IN MORE DEPTH

Explain pooling rules (variance estimates + adjusted variance estimates + simple estimator proposed by synthpop authors)

## Partially synthetic data

DISCUSS APPROACH IN MORE DEPTH

Explain pooling rules (variance estimates)

# Mice

- Discuss the FCS algorithm in more detail.
- Discuss mice, predominantly the imputation algorithm in greater depth
- Procedure: observed values can be regarded as starting values, so that all variables can be used for the synthesis of all other variables.

# Simulations

## Methods

- Boys data - bootstrapped to obtain a population.
- explain simple synthetic data procedure
- explain imputation settings
- CART has been proposed by Reiter


## Results





## References

