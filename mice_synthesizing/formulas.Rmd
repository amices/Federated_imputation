---
title: "Formulas"
authors: 
- Thom Volker
- Utrecht University
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Missing data

Inferences for an unknown scalar parameter $Q$, say, a regression coefficient, are based on a point estimate $q$, a variance estimate $u$, and a normal or Student's $t$ reference distribution. For analysis of the imputed datasets, let $q^{(i)}$ and $u^{(i)}$ for $i = 1, 2, \dots, m$ be the point and variance estimates achieved from each of the $m$ completed datasets. To get a final estimate over all imputations, these estimates have to be combined using the combining rules described by Rubin (1978). The following quantities are needed for inferences for scalar $Q$:
$$
\begin{aligned}
\bar{q}_m &= \sum_{i = 1}^m \frac{q^{(i)}}{m}, \\
b_m &= \sum_{i = 1}^m \frac{(q^{(i)} - \bar{q}_m)^2}{m-1}, \\
\bar{u}_m &= \sum_{i = 1}^m \frac{u^{(i)}}{m}.
\end{aligned}
$$
The analyst then can use $\bar{q}_m$ to estimate $Q$ and
$$
T_m = \bar{u}_m + (1 + \frac{1}{m}) b_m
$$
to estimate the variance of $\bar{q}_m$. Then, $\bar{u}_m$ can be regarded as the "within-imputation" variance, and $b_m$ as the "between-imputation" variance. The factor $(1 + \frac{1}{m})$ reflects the fact that only a finite number of completed-data estimates $q^{(i)}$ are averaged together to obtain the final point estimate.

The quantity $r = (1 + \frac{1}{m}) \frac{b_m}{T_m}$ estimates the fraction of information about $Q$ that is missing due to nonresponse. Inferences from multiply imputed data are based on $\bar{q}_m$, $T_m$, and a Student's $t$ reference distribution. Rubin and Schenker (1986) provide the approximate value $\nu_{RS} = \frac{m - 1}{r^2}$ for the degrees of freedom of the $t$ distribution under the assumption that with complete data a normal reference distribution would have been appropriate. Barnard and Rubin (1999) relax the assumption of Rubin and Schenker (1986) to allow for a $t$ reference distribution with complete data and suggest the value $\nu_{BR} = (\frac{1}{\nu_{RS}} + \frac{1}{\hat{\nu}_{obs}})^{-1}$ for the degrees of freedom in the multiple imputation analysis, where $\hat{\nu}_{obs} = \frac{(1- r)(\nu_{com})(\nu_{com} + 1)}{\nu_{com} + 1}$ and $\nu_{com}$ denotes the complete-data degrees of freedom.


---


# Fully synthetic datasets

We again have scalar population parameter $Q$, with $q^{(i)}$ and $u^{(i)}$ as it's estimates from each of the $m$ synthetic datasets. The following quantities are needed for inferences for scalar $Q$:

$$
\begin{aligned}
\bar{q}_m &= \sum_{i=1}^m \frac{q^{(i)}}{m}, \\
b_m &= \sum_{i=1}^m \frac{(q^{(i)} - \bar{q}_m)^2}{m-1}, \\
\bar{u}_m &= \sum_{i=1}^m \frac{u^{(i)}}{m}.
\end{aligned}
$$

The analyst then can use $\bar{q}_m$ to estimate $Q$ and 
$$
T_f = (1 + \frac{1}{m})b_m - \bar{u}_m
$$
to estimate the variance of $\bar{q}_m$. The difference in this variance estimate compared with the variance estimate for standard multiple imputation is due to the additional sampling from the synthetic units for fully synthetic datasets. Hence, the variance $b_m$ between the datasets already reflects the variance within each imputation. When $n$ is large, inferences for scalar $Q$ can be based on $t$ distributions with degrees of freedom 
$$
\nu_f = (m - 1) \Bigg{(}1 - \frac{\bar{u}_m}{(1 + \frac{1}{m})b_m}\Bigg{)}^2.
$$
Derivations of these methods are presented in Raghunathan et al. (2003). A disadvantage of this variance estimate is that it can become negative. For that reason, Reiter (2002) suggests a slightly modified variance estimator that is always positive $T_f^{*} = \text{max}(0, T_f) + \delta \Big{(}\frac{n_{syn}}{n}\bar{u}_m \Big{)}$, where $\delta = 1$ if $T_f < 0$ and $\delta = 0$ otherwise.


---


# Partially synthetic datasets

Following Reiter (2003, 2004), let $Z_j = 1$ if unit $j$ is selected to have any of its observed data replaced, and let $Z_j = 0$ otherwise. Let $Z = (Z_1, \dots, Z_s)$, where $s$ is the number of records in the observed data. Let $Y = (Y_{rep}, Y_{nrep})$ be the data collected in the original survey, where $Y_{rep}$ includes all values to be replaced with multiple imputations and $Y_{nrep}$ includes all values not replaced with imputations. Let $Y_{rep}^{(i)}$ be the replacement values for $Y_{rep}$ in synthetic dataset $i$. Each $Y_{rep}^{(i)}$ is generated by simulating values from the posterior predictive distribution $f(Y_{rep}^{(i)} | Y, Z)$, or some close approximation to the distribution such as those of Raghunathan et al. (2001). The agency repeats the process $m$ times, creating $D^{(i)} = (Y_{nrep}, Y_{rep}^{(i)})$ for $i = 1, \dots, m$, and releases $\textbf{D} = \{D^{(1)}, \dots, D^{(m)}\}$ to the public.

To get valid inferences, secondary data users can use the combining rules presented by Reiter (2003). For scalar $Q$, with $q^{(i)}$ and $u^{(i)}$ the point estimate and the corresponding variance estimate in synthetic dataset $D^{(i)}$ for $i = 1, \dots, m$, the following quantities are needed for inferences:

$$
\begin{aligned}
\bar{q}_m &= \sum_{i=1}^m \frac{q^{(i)}}{m}, \\
b_m &= \sum_{i=1}^m \frac{(q^{(i)} - \bar{q}_m)}{m-1}, \\
\bar{u}_m &= \sum_{i=1}^m \frac{u^{(i)}}{m}.
\end{aligned}
$$

The analyst can use $\bar{q}_m$ to estimate $Q$ and 
$$
T_p = \frac{b_m}{m} + \bar{u}_m
$$
to estimate the variance of $\bar{q}_m$. Then, $\frac{b_m}{m}$ is the correction factor for the additional variance due to using a finite number of imputations. However, the additional $b_m$ necessary in the missing-data context is not necessary here, since $\bar{u}_m$ already captures the variance of $Q$ given the observed data. This is different in the missing data case, where $\bar{u}_m$ is the variance of $Q$ given the completed data, and $\bar{u} + b_m$ is the variance of $Q$ given the observed data. When $n$ is large, inferences for scalar $Q$ can be based on $t$ distributions with degrees of freedom $\nu_p = (m-1) \Big{(}1 + \frac{\bar{u}_m \cdot m}{b_m}\Big{)}^2.$


---


# Nonresponse and partially synthetic data

Since the generation of synthetic datasets is based on the ideas of multiple imputation, it is reasonable to use the approach to impute missing values and generate synthetic values simultaneously. At first glance, it seems logical to impute missing values and generate synthetic values in one step using the same model as for the originally observed values. However, as Reiter (2004) points out, this can lead to biased imputations of only a subset of the data should be replaced with synthetic values, but the imputation model for the missing values is based on the entire dataset. To allow for different models, Reiter (2004) suggests imputation in two stages. In the first stage, all missing values are imputed $m$ times using the standard multiple-imputation approach for nonresponse. In the second stage, all values that need to be replaced are synthesized $r$ times in every first-stage nest, leading to a total of $M = m \cdot r$ datasets that are released to the public.

Again, let $Q$ be an estimand, and suppose that, given the original data, the analyst estimates $Q$ with some point estimator $q$ and the variance of $q$ with some estimator $u$. Let $q^{(i,j)}$ and $u^{(i,j)}$ be the values of $q$ and $u$ in synthetic dataset $D^{(i,j)}$ for $i = 1, \dots, m$ and $j = 1, \dots, r$. The analyst computes $q^{(i,j)}$ and $u^{(i,j)}$ by acting as if each $D^{(i,j)}$ is the genuine data. The following quantities are needed for inferences for scalar $Q$:

$$
\begin{aligned}
\bar{q}_M &= \sum_{i=1}^m \sum_{j=1}^r \frac{q^{(i,j)}}{m \cdot r} = \sum_{i=1}^m \frac{\bar{q}^{(i)}}{m}, \\
\bar{b}_M &= \sum_{i=1}^m \sum_{j=1}^r \frac{(q^{(i,j)} - \bar{q}^{(i)})^2}{m(r-1)} = \sum_{i=1}^m \frac{b^{(i)}}{m}, \\
B_M &= \sum_{i=1}^m \frac{(\bar{q}^{(i)}) - \bar{q}_M}{m-1}, \\
\bar{u}_M = \sum_{i=1}^m \sum_{j=1}^r \frac{u^{i,j}}{m \cdot r}.
\end{aligned}
$$

The analyst then can use $\bar{q}_M$ to estimate $Q$ and
$$
T_P = (1 + \frac{1}{m}) B_M - \frac{\bar{b}_M}{r} + \bar{u}_M
$$
to estimate the variance of $\bar{q}_M$. When $n$ is large, inferences for scalar $Q$ can be based on $t$ distributions with degrees of freedom 
$$
\nu_P = \Bigg{(}\frac{((1 + \frac{1}{m})B_M)^2}{(m-1)T_M^2} + \frac{(\frac{\bar{b}_M}{r})^2}{m(r-1)T_M^2} \Bigg{)}^{-1}.
$$

Similar to the variance of the estimate for fully synthetic datasets, $T_P$ can become negative since $\frac{\bar{b}_M}{r}$ is substracted. In this case, Reiter (2008b) suggests using the conservative variance estimator $T_P^{adj} = (1 = \frac{1}{m})B_M + \bar{u}_M$. This estimator is equivalent to the variance estimator for multiple imputation for missing data. Consequently, the degrees of freedom are given by 
$$
\nu_P^{adj} = (m - 1) \Bigg{(}1 + \frac{m \bar{u}_M)}{(m+1)B_M} \Bigg{)}^2.
$$
Generally, negative variances can be avoided by increasing $m$ and $r$.


---


END OF DOCUMENT

