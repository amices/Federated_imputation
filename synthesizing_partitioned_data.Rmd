---
title: "Federated Imputation"
author: 
- Thom Volker
- Utrecht University
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
bibliography: federated_imp.bib
csl: "/Users/thomvolker/Documents/styles/apa-6th-edition.csl"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
thomcache <- TRUE; thomlazy <- FALSE #obviously ;)
library(tidyverse)
```

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.d Synthesize partitioned data.R")
```


# Previous results

Without partitioning the data, we already saw that synthesizing does not distort the relationships of the variables in the data at hand. That is, synthesizing a dataset results in $M$ synthetic datasets with, on average, the same properties as the original dataset. The results can be seen in the following table. This table contains the simulation results of synthesizing multivariate normally distributed data, where the correlation between predictors equals $\rho = 0$, the proportion explained variance equals $R^2 = .50$ and the sample size equals $n = 100$. Note that synthesizing is done by means of the default settings (by means of CART), and the variance of the estimates is calculated as 
$$Var(\beta) = \bar{v}_{M}(1 + \frac{1}{M}),$$
where $\bar{v}_M$ is the average variance of the regression coefficient over the $M$ synthetic datasets.

```{r, echo = F, cache=thomcache, cache.lazy=thomlazy}
knitr::kable(summary_out_cart, digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data with m=5 in the last three rows.")
```

# Results with partitioned data and uncorrelated predictors

## CART Synthesizing method

CART Method: First variable is sampled from the observed data, the second variable is predicted from the first variable by means of a regression tree, the third variable is predicted from the first two variables, and so on. 

When we first partition the sampled data from the multivariate normal distribution, the results change to quite some extent, which can be seen in the following table. The coverage seems to be very low, and far below the nominal level of $95\%$. 


```{r, echo = F, cache=thomcache, cache.lazy=thomlazy}
bind_rows("n = 100" = summary_out_cart_part_n100, 
          "n = 1000" = summary_out_cart_part_n1000,
          "n = 10000" = summary_out_cart_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```


## Norm Synethesizing method

Norm Method: The first variable is sampled from a univariate normal distribution, the second variable is drawn from a normal distribution with mean $\mu = \beta_1 * X_1$ and variance $\sigma^2 = \sigma^2_e$, the third variable is drawn based on the first two variables, and so on. This performs much better than the previous method, but this introduces bias, which is accompanied by a somewhat too low coverage.

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
bind_rows("n = 100" = summary_out_norm_part_n100,
          "n = 1000" = summary_out_norm_part_n1000, 
          "n = 10000" = summary_out_norm_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of method 'norm' with M = 10 in the last three rows with partitioned data.")
  

```


# Results with partitioned data and correlated predictors

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.e Synthesize partitioned data with correlations.R")
```

When there are no correlations between the predictors, there is less information in the data with regard to every predictor, so it might be more difficult to correctly estimate all relationships in the data appropriately. Therefore, we introduce some moderate correlations between the predictors, so that, hopefully, the patterns in the data can be used to synthesize the data. The following correlation matrix is used to model the relationships between the data.

$$
\rho =
\begin{bmatrix}
1 & 0.15 & 0.25\\
0.15 & 1 & 0.35\\
0.25 & 0.35 & 1
\end{bmatrix}
$$

## CART Synthesizing method

When we use the CART synthesizing method, in combination with the data with correlated predictors coming from a multivariate normal distribution, we obtain the following results.

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
bind_rows("n = 100" = cor_summary_out_cart_part_n100, 
          "n = 1000" = cor_summary_out_cart_part_n1000,
          "n = 10000" = cor_summary_out_cart_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

## Norm Synthesizing method

When we use the norm synthesizing method, in combination with the data with correlated predictors coming from a multivariate normal distribution, we obtain the following results.

```{r, echo = F, cache = thomcache, cache.lazy=thomlazy}
bind_rows("n = 100" = cor_summary_out_norm_part_n100, 
          "n = 1000" = cor_summary_out_norm_part_n1000,
          "n = 10000" = cor_summary_out_norm_part_n10000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

# Ten subsets instead of five

## Uncorrelated predictors

### CART Synthesization

Below, the results are presented when the data is partitioned into 10 subsets, instead of five, corresponding to ten different nodes that are synthesized separately, and then combined into a single dataset that is analysed by means of the standard estimators as presented by @raab_practical_2016. This yields regression coefficients that are equal to
$$
\beta_{M,k} = \sum^M_{m=1} \beta_{m,k} / M,
$$
with a variance of 
$$
V = \bar{v}_M(1+\frac{1}{M}),
$$
where $\bar{v}_M$ equals $\sum^M_{m=1}\frac{v_m}{M}$.

```{r, include = F, cache=thomcache, cache.lazy=thomlazy}
source("2.f Synthesize 10 subsets with and without correlations.R")
```


```{r, echo = F}
bind_rows("n = 100" = p10_summary_out_cart_part_n100, 
          "n = 1000" = p10_summary_out_cart_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

### Norm synthesization

```{r, echo = F}
bind_rows("n = 100" = p10_summary_out_norm_part_n100, 
          "n = 1000" = p10_summary_out_norm_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

## Correlated predictors

### CART synthesization

```{r, echo = F}
bind_rows("n = 100" = cor_p10_summary_out_cart_part_n100, 
          "n = 1000" = cor_p10_summary_out_cart_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

### Norm synthesization

```{r, echo = F}
bind_rows("n = 100" = cor_p10_summary_out_norm_part_n100, 
          "n = 1000" = cor_p10_summary_out_norm_part_n1000, .id = "Sample size") %>%
  knitr::kable(., digits = 3, caption = "Results of a linear regression model where DV is regression on the predictors IV1, IV2 and IV3, with the values sampled directly from the population in the first three rows, and the synthesized data by means of CART with M = 10 in the last three rows with partitioned data.")
```

# Workflow from now on

Necessity to further investigate structure of the data by means of, say, eigenvalues (correlations seem to be okay with large samples, according to results presented)?

## Proposal

@. Verder werken met de completed boys data (ondanks de hoge correlatie sowieso realistischer dan gesimuleerde multivariate normal data). Eventueel zou ik ook nog een andere dataset kunnen zoeken die wellicht meer lijkt op een "standaard" social sciences dataset.
  + Indien we met de boys data werken, kunnen we eventueel ook nog een subset nemen (om te kijken hoe het gaat met "echte" data als we de sample size verkleinen), en we zouden kunnen bootstrappen om een grotere sample te creÃ«ren dan de huidige 748 observaties (wellicht met Stef overleggen wat een redelijk aantal observaties en nodes zijn over verschillende ziekenhuizen).
@. Boys dataset bootstrappen, zodat we een "superpopulatie" krijgen. En dan zowel gebootstrapte echte data vergelijken met synthetische versie van elke boodstrap sample. In dat geval dat geval is het gemiddelde over de bootstrap samples de populatie waarde. 
  + 
1. Second point.
